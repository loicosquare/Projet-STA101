library(fastkmedoids)
library("umap")
library(cluster)
library(dplyr)
library(clustrd)
install.packages("clustrd")
library("readxl")
library("FactoMineR")
library("factoextra")
library(Factoshiny)
library(NbClust)
library(caret)
library(mclust)
library(ggplot2)
library(stats)
library(factoextra)
library(fpc)
library(cluster)
#library(spectralCluster)
#library(anocva)
library(kernlab)
library(dbscan)
library(aricode)
library(skmeans)
library(fastkmedoids)
library("umap")
library(cluster)
library(dplyr)
library(clustrd)
install.packages("gridExtra")
library("readxl")
library("FactoMineR")
library("factoextra")
library(Factoshiny)
library(NbClust)
library(caret)
library(mclust)
library(ggplot2)
library(stats)
library(factoextra)
library(fpc)
library(cluster)
#library(spectralCluster)
library(grid)
library(kernlab)
library(dbscan)
library(aricode)
library(skmeans)
library(fastkmedoids)
library("umap")
library(cluster)
library(dplyr)
library(clustrd)
clusters <- reduced_kmeans(data, k = 20, method="pca", ncomp = 2)
cluspca(data, 20, ndim, alpha = NULL, method = "RKM",
center = TRUE, scale = TRUE, rotation = "none", nstart = 100,
smartStart = NULL, seed = NULL)
cluspca(data, 20, alpha = NULL, method = "RKM",
center = TRUE, scale = TRUE, rotation = "none", nstart = 100,
smartStart = NULL, seed = NULL)
cluspca(data, 20, 2, alpha = 0.01, method = "RKM",
center = TRUE, scale = TRUE, rotation = "varimax", nstart = 1000)
cluspca(data, 20, 2, alpha = 0.01, method = "RKM",
center = TRUE, scale = TRUE, rotation = "varimax", nstart = 1000)
cluspca(data, 20, 2, alpha = 0.01, method = "RKM",
center = TRUE, scale = TRUE, rotation = "varimax", nstart = 100)
cluspca(data, 20, 2, alpha = 0.01, method = "RKM",
center = TRUE, scale = TRUE, rotation = "varimax", nstart = 10)
cluspca(data, 20, 2, alpha = 0.01, method = "RKM", rotation = "varimax", nstart = 10)
library("readxl")
library("FactoMineR")
library("factoextra")
library(Factoshiny)
library(NbClust)
library(caret)
library(mclust)
library(ggplot2)
library(stats)
library(factoextra)
library(fpc)
library(cluster)
#library(spectralCluster)
library(grid)
library(kernlab)
library(dbscan)
library(aricode)
library(skmeans)
library(fastkmedoids)
library("umap")
library(cluster)
library(dplyr)
library(clustrd)
library(RSKC)
library(EMMIXmfa)
algoFinal <- data.frame(matrix(ncol = 4))
columns <- c(
"NOM ALGORITHME",	"NMI",	"ARI", "COMMENTAIRE"
)
colnames(algoFinal) <- columns
current_algorihtm <- algoFinal
algoFinal <- algoFinal[-1,]
saveResults <- function(nomAlgo, nmi, ari, commnt){
current_algorihtm$`NOM ALGORITHME` <- nomAlgo
current_algorihtm$NMI <- round(nmi,2)
current_algorihtm$ARI <- round(ari,2)
current_algorihtm$COMMENTAIRE <- commnt
#sauvegarder les résultats obtenus
algoFinal <<- rbind(algoFinal, current_algorihtm)
#réinitailiser la variable
current_algorihtm <- data.frame(matrix(ncol = 4))
colnames(current_algorihtm) <- columns
}
data <- read.table("jose_d.txt", sep = "", na.strings = "", stringsAsFactors = F, skip = 1, fill = T)
data <- data[,2:ncol(data)]
data.kmeans <- kmeans(data,20, iter.max = 25)
fviz_cluster(data.kmeans, data = data)
predicted_labels <- data.kmeans$cluster - 1
true_labels <- read.table("label.txt", sep = "", na.strings = "", stringsAsFactors = F, fill = T)
saveResults("KMEANS", NMI(as.factor(predicted_labels), as.factor(true_labels$V1) ), ARI(as.factor(predicted_labels), as.factor(true_labels$V1)), " Lorsque nous analysons les résultats obtenus après avoir appliqué k-means, nous pouvons voir les clusters qui sont séparés par des couleurs differentes mais, par contre ces résultats ne sont pas très satisfaisants comme nous pouvons le constater. malgré que la NMI est au dessus de 0.5, Il est donc important de noter que, même si une NMI élevée est souvent considérée comme indiquant une bonne qualité de clustering, cela ne signifie pas nécessairement que les clusters obtenus sont pertinents.")
res.sph_kmeans <- skmeans(as.matrix(data),20)
saveResults("SPHERICAL KMEANS", NMI(res.sph_kmeans$cluster - 1,true_labels$V1), ARI(res.sph_kmeans$cluster - 1,true_labels$V1), "Les résultats de sphérical Kmeans sont pratiquement identiques (cela signifie que les deux algorithmes ont produit des clusters similaires en termes de qualité) à ceux obtenus avec Kmeans nous pouvons expliquer celà par le fait que : Cela peut être dû au fait que les données sont déjà sphériques ou qu'elles ont une distribution gaussienne, et que l'utilisation de l'algorithme Spherical k-means n'apporte donc pas de bénéfice supplémentaire par rapport à l'algorithme k-means standard. Il est également possible que les deux algorithmes ont convergé vers les mêmes résultats. De plus, la NMI n'est qu'une des métriques d'évaluation de la qualité de clustering, il est donc important de considérer d'autres indicateurs pour évaluer les résultats obtenus avec les deux algorithmes.")
BIC <- mclustBIC(data, G = 20)
summary(BIC, parameters = TRUE)
mclust = Mclust(data, x = BIC)
ayers <- 2
k <- c(3, 4)
r <- c(3, 2)
it <- 50
eps <- 0.001
set.seed(1)
fit <-deepgmm(y = data, layers = layers, k = k, r = r,it = it, eps = eps)
install.packages("deepgmm")
install.packages("deepgmm")
library("readxl")
library("FactoMineR")
library("factoextra")
library(Factoshiny)
library(NbClust)
library(caret)
library(mclust)
library(ggplot2)
library(stats)
library(factoextra)
library(fpc)
library(cluster)
#library(spectralCluster)
library(grid)
library(kernlab)
library(dbscan)
library(aricode)
library(skmeans)
library(fastkmedoids)
library("umap")
library(cluster)
library(dplyr)
library(clustrd)
library(RSKC)
library(EMMIXmfa)
library(deepgmm)
ayers <- 2
k <- c(3, 4)
r <- c(3, 2)
it <- 50
eps <- 0.001
set.seed(1)
fit <-deepgmm(y = data, layers = layers, k = k, r = r,it = it, eps = eps)
layers <- 2
k <- c(3, 4)
r <- c(3, 2)
it <- 50
eps <- 0.001
set.seed(1)
fit <-deepgmm(y = data, layers = layers, k = k, r = r,it = it, eps = eps)
View(fit)
summary(fit)
View(estimated_model)
Y <- data
mfa_model <- mfa(Y, g = 3, q = 3)
layers <- 2
k <- c(3, 4)
r <- c(3, 2)
it <- 50
eps <- 0.001
set.seed(1)
fit <-deepgmm(y = data, layers = layers, k = k, r = r,it = it, eps = eps)
summary(fit)
library("readxl")
library("FactoMineR")
library("factoextra")
library(Factoshiny)
library(NbClust)
library(caret)
library(mclust)
library(ggplot2)
library(stats)
library(factoextra)
library(fpc)
library(cluster)
#library(spectralCluster)
library(grid)
library(kernlab)
library(dbscan)
library(aricode)
library(skmeans)
library(fastkmedoids)
library("umap")
library(cluster)
library(dplyr)
library(clustrd)
library(RSKC)
library(EMMIXmfa)
library(deepgmm)
algoFinal <- data.frame(matrix(ncol = 4))
columns <- c(
"NOM ALGORITHME",	"NMI",	"ARI", "COMMENTAIRE"
)
colnames(algoFinal) <- columns
current_algorihtm <- algoFinal
algoFinal <- algoFinal[-1,]
saveResults <- function(nomAlgo, nmi, ari, commnt){
current_algorihtm$`NOM ALGORITHME` <- nomAlgo
current_algorihtm$NMI <- round(nmi,2)
current_algorihtm$ARI <- round(ari,2)
current_algorihtm$COMMENTAIRE <- commnt
#sauvegarder les résultats obtenus
algoFinal <<- rbind(algoFinal, current_algorihtm)
#réinitailiser la variable
current_algorihtm <- data.frame(matrix(ncol = 4))
colnames(current_algorihtm) <- columns
}
data <- read.table("jose_d.txt", sep = "", na.strings = "", stringsAsFactors = F, skip = 1, fill = T)
#PCAshiny(data)
data <- data[,2:ncol(data)]
data.kmeans <- kmeans(data,20, iter.max = 25)
fviz_cluster(data.kmeans, data = data)
predicted_labels <- data.kmeans$cluster - 1
true_labels <- read.table("label.txt", sep = "", na.strings = "", stringsAsFactors = F, fill = T)
saveResults("KMEANS", NMI(as.factor(predicted_labels), as.factor(true_labels$V1) ), ARI(as.factor(predicted_labels), as.factor(true_labels$V1)), " Lorsque nous analysons les résultats obtenus après avoir appliqué k-means, nous pouvons voir les clusters qui sont séparés par des couleurs differentes mais, par contre ces résultats ne sont pas très satisfaisants comme nous pouvons le constater. malgré que la NMI est au dessus de 0.5, Il est donc important de noter que, même si une NMI élevée est souvent considérée comme indiquant une bonne qualité de clustering, cela ne signifie pas nécessairement que les clusters obtenus sont pertinents.")
res.sph_kmeans <- skmeans(as.matrix(data),20)
saveResults("SPHERICAL KMEANS", NMI(res.sph_kmeans$cluster - 1,true_labels$V1), ARI(res.sph_kmeans$cluster - 1,true_labels$V1), "Les résultats de sphérical Kmeans sont pratiquement identiques (cela signifie que les deux algorithmes ont produit des clusters similaires en termes de qualité) à ceux obtenus avec Kmeans nous pouvons expliquer celà par le fait que : Cela peut être dû au fait que les données sont déjà sphériques ou qu'elles ont une distribution gaussienne, et que l'utilisation de l'algorithme Spherical k-means n'apporte donc pas de bénéfice supplémentaire par rapport à l'algorithme k-means standard. Il est également possible que les deux algorithmes ont convergé vers les mêmes résultats. De plus, la NMI n'est qu'une des métriques d'évaluation de la qualité de clustering, il est donc important de considérer d'autres indicateurs pour évaluer les résultats obtenus avec les deux algorithmes.")
D = kNNdist(as.data.frame(data)  ,k=20,search="kd")
plot(sort(D,decreasing=TRUE),type="l",xlim=c(0,1000),ylim=c(0,10))
abline(h=0.15,col="red",lty=2)
x <- as.matrix(data)
res.dbscan <- dbscan(x, eps = 1.2, minPts = 10)
res.dbscan_nmi <- NMI(res.dbscan$cluster - 1,true_labels$V1)
res.dbscan_nmi
saveResults("DBSCAN", NMI(res.dbscan$cluster - 1,true_labels$V1), ARI(res.dbscan$cluster - 1,true_labels$V1), "Cet algorithme retourne soit 1 soit 2 clusters, nous pouvons dire qu'il faut utiliser d'autres techniques pour le meilleur choix des paramètre ou alors nos données ne sont pas suffisamment densément groupées or, l'algorithme se base sur la densité afin de retourner plusieurs clusters ou aucun cluster si les données ne remplissent pas les critères de densité nécessaires (qui pourrait probablement être notre cas).")
res.dbscan
dist <- as.vector(dist(data))
#res.pam <- pam(dist, nrow(data), k=20)
clara <- clara(data, 20, samples = 50, pamLike = TRUE)
clara_clust <- clara$clustering
saveResults("CLARA", NMI(as.factor(clara_clust), as.factor(true_labels$V1) ), ARI(as.factor(clara_clust), as.factor(true_labels$V1) ), "")
res.fastpam <- fastpam(dist,nrow(data), 20)
saveResults("FAST PAM", NMI(as.factor(res.fastpam@assignment - 1), as.factor(true_labels$V1) ), ARI(as.factor(res.fastpam@assignment - 1), as.factor(true_labels$V1) ), "")
res.fastclara <- fastclara(dist, nrow(data), k=20, numsamples = 5, sampling=0.25)
saveResults("FAST CLARA", NMI(as.factor(res.fastclara@assignment - 1), as.factor(true_labels$V1) ), ARI(as.factor(res.fastclara@assignment - 1), as.factor(true_labels$V1) ), "Nous pouvons donc constater que Fast Clara donne de meilleur résultats que Fast Clarans")
res.fastclarans <- fastclarans(dist, nrow(data), 20, numlocal=2, maxneighbor=0.025)
saveResults("FAST CLARANS", NMI(as.factor(res.fastclarans@assignment - 1), as.factor(true_labels$V1) ), ARI(as.factor(res.fastclarans@assignment - 1), as.factor(true_labels$V1) ), "Nous pouvons donc constater que Fast Clara donne de meilleur résultats que Fast Clarans")
#dat <-matrix(unlist(data), ncol=ncol(data), byrow=TRUE)
#result <- specc(dat, centers = 20)
#saveResults("SPECTRAL CLUSTERING", NMI(as.factor(result), as.factor(true_labels$V1)), ARI(as.factor(result), as.factor(true_labels$V1)), "")
saveResults("SPECTRAL CLUSTERING", 0, 0, "L'algorithme Spectral clustering tourne des heures sans jamais s'arrêter, nous l'avons lancé toute une nuit sans obtenir de résultats le problème peut être du à : La taille des données : Si les données sont très volumineuses, l'algorithme peut prendre beaucoup de temps, le nombre de clusters demandé est très élevé, cela peut rendre l'algorithme très complexe et prendre beaucoup de temps pour s'exécuter.")
r <- RSKC(data, ncl = 20, alpha = 0.1, nstart = 10)
saveResults("SPARSE KMEANS", NMI(as.factor(r$labels), as.factor(true_labels$V1) ), ARI(as.factor(r$labels), as.factor(true_labels$V1)), "Sparse Kmeans donne de bons résultats, nous pouvons expliquer celà par le fait que le partitionnement des données permet de mieux gérer les outliers et les points atypiques, ce qui améliore la qualité des clusters formés.le partitionnement peut également aider à identifier des structures non linéaires dans les données, ce qui améliore la qualité des clusters formés.")
data.umap <- umap(data, n_neighbors = 20, min_dist = 0.0001)
res.umap <- data.umap$layout %>% as.data.frame()%>% rename(UMAP1="V1", UMAP2="V2")
data.kmeans.umap <- kmeans(res.umap ,20)
saveResults("UMAP + KMEANS", NMI(as.factor(data.kmeans.umap$cluster - 1), as.factor(true_labels$V1) ), ARI(as.factor(data.kmeans.umap$cluster - 1), as.factor(true_labels$V1) ), "UMAP + Kmeans fonctionne mieux que ACP + Kmeans car UMAP est un algorithme de réduction de dimension qui est spécifiquement conçu pour conserver les propriétés de similarité des données d'origine tout en réduisant les dimensions. Il utilise un processus de mappage uniforme pour projeter les données dans un espace de dimension réduite, en gardant les observations similaires proches les unes des autres dans la projection.
En revanche, l'ACP est un algorithme de réduction de dimension qui se concentre sur la capture de la variation maximale des données d'origine dans un nombre limité de dimensions. Il peut ne pas conserver les propriétés de similarité des données d'origine de la même manière que UMAP, ce qui peut entraîner une perte d'informations importante ceci pourrait expliquer le fait que les résultats obtenus avec UMAP sont meilleurs.")
fviz_cluster(data.kmeans.umap, data = res.umap, geom="point")
NMI(data.kmeans.umap$cluster - 1,true_labels$V1)
data.acp <- PCA(data, ncp = 15, graph = TRUE)
data.kmeans.acp <- kmeans(data.acp$call$X ,20)
saveResults("ACP + KMEANS", NMI(as.factor(data.kmeans.acp$cluster - 1), as.factor(true_labels$V1) ), ARI(as.factor(data.kmeans.acp$cluster - 1), as.factor(true_labels$V1) ), "UMAP + Kmeans fonctionne mieux que ACP + Kmeans car UMAP est un algorithme de réduction de dimension qui est spécifiquement conçu pour conserver les propriétés de similarité des données d'origine tout en réduisant les dimensions. Il utilise un processus de mappage uniforme pour projeter les données dans un espace de dimension réduite, en gardant les observations similaires proches les unes des autres dans la projection.
En revanche, l'ACP est un algorithme de réduction de dimension qui se concentre sur la capture de la variation maximale des données d'origine dans un nombre limité de dimensions. Il peut ne pas conserver les propriétés de similarité des données d'origine de la même manière que UMAP, ce qui peut entraîner une perte d'informations importante ceci pourrait expliquer le fait que les résultats obtenus avec UMAP sont meilleurs.")
fviz_cluster(data.kmeans.acp, data = data.acp$call$X)
plot(data.acp$call$X[,1:2], col=data.kmeans.acp$cluster)
points(data.kmeans.acp$centers[,1:2], col=1:3, pch=8, cex=3)
clusters <- cluspca(data, 20, method = "RKM")
View(algoFinal)
Y <- data
mfa_model <- mfa(Y, g = 3, q = 3)
algoFinal
NMI(clusters$cluster - 1,true_labels$V1)
NMI(clusters$cluster - 1,true_labels$V1)
NMI(clusters$cluster - 1,true_labels$V1)
NMI(clusters$cluster - 1,true_labels$V1)
NMI(clusters$cluster - 1,true_labels$V1)
NMI(clusters$cluster - 1,true_labels$V1)
NMI(clusters$cluster - 1,true_labels$V1)
NMI(clusters$cluster - 1,true_labels$V1)
NMI(clusters$cluster - 1,true_labels$V1)
NMI(clusters$cluster - 1,true_labels$V1)
NMI(clusters$cluster - 1,true_labels$V1)
NMI(clusters$cluster - 1,true_labels$V1)
NMI(clusters$cluster - 1,true_labels$V1)
View(algoFinal)
saveResults("REDUCED KMEANS + ACP", NMI(as.factor(clusters$cluster - 1), as.factor(true_labels$V1) ), ARI(as.factor(clusters$cluster - 1), as.factor(true_labels$V1) ), "Reduced Kmeans donne pratiquement les même résultats que Kmeans malgré qu'il applique une ACP, nous pouvons expliquer celà par le fait que l'ACP ne conserve pas les propriétés de similarité des données d'origine, elle se concentre sur la capture de la variation maximale des données d'origine dans un nombre limité de dimensions")
algoFinal
library("readxl")
library("FactoMineR")
library("factoextra")
library(Factoshiny)
library(NbClust)
library(caret)
library(mclust)
library(ggplot2)
library(stats)
library(factoextra)
library(fpc)
library(cluster)
#library(spectralCluster)
library(grid)
library(kernlab)
library(dbscan)
library(aricode)
library(skmeans)
library(fastkmedoids)
library("umap")
library(cluster)
library(dplyr)
library(clustrd)
library(RSKC)
library(EMMIXmfa)
library(deepgmm)
algoFinal <- data.frame(matrix(ncol = 4))
columns <- c(
"NOM ALGORITHME",	"NMI",	"ARI", "COMMENTAIRE"
)
colnames(algoFinal) <- columns
current_algorihtm <- algoFinal
algoFinal <- algoFinal[-1,]
Y <- data
mfa_model <- mfa(Y, g = 3, q = 3)
mtfa_model <- mtfa(Y, g = 3, q = 3)
library("readxl")
library("FactoMineR")
library("factoextra")
library(Factoshiny)
library(NbClust)
library(caret)
library(mclust)
library(ggplot2)
library(stats)
library(factoextra)
library(fpc)
library(cluster)
#library(spectralCluster)
library(grid)
library(kernlab)
library(dbscan)
library(aricode)
library(skmeans)
library(fastkmedoids)
library("umap")
library(cluster)
library(dplyr)
library(clustrd)
library(RSKC)
library(EMMIXmfa)
library(deepgmm)
algoFinal <- data.frame(matrix(ncol = 4))
columns <- c(
"NOM ALGORITHME",	"NMI",	"ARI", "COMMENTAIRE"
)
colnames(algoFinal) <- columns
current_algorihtm <- algoFinal
algoFinal <- algoFinal[-1,]
saveResults <- function(nomAlgo, nmi, ari, commnt){
current_algorihtm$`NOM ALGORITHME` <- nomAlgo
current_algorihtm$NMI <- round(nmi,2)
current_algorihtm$ARI <- round(ari,2)
current_algorihtm$COMMENTAIRE <- commnt
#sauvegarder les résultats obtenus
algoFinal <<- rbind(algoFinal, current_algorihtm)
#réinitailiser la variable
current_algorihtm <- data.frame(matrix(ncol = 4))
colnames(current_algorihtm) <- columns
}
data <- read.table("jose_d.txt", sep = "", na.strings = "", stringsAsFactors = F, skip = 1, fill = T)
#PCAshiny(data)
data <- data[,2:ncol(data)]
fviz_cluster(data.kmeans, data = data)
predicted_labels <- data.kmeans$cluster - 1
true_labels <- read.table("label.txt", sep = "", na.strings = "", stringsAsFactors = F, fill = T)
saveResults("KMEANS", NMI(as.factor(predicted_labels), as.factor(true_labels$V1) ), ARI(as.factor(predicted_labels), as.factor(true_labels$V1)), " Lorsque nous analysons les résultats obtenus après avoir appliqué k-means, nous pouvons voir les clusters qui sont séparés par des couleurs differentes mais, par contre ces résultats ne sont pas très satisfaisants comme nous pouvons le constater. malgré que la NMI est au dessus de 0.5, Il est donc important de noter que, même si une NMI élevée est souvent considérée comme indiquant une bonne qualité de clustering, cela ne signifie pas nécessairement que les clusters obtenus sont pertinents.")
res.sph_kmeans <- skmeans(as.matrix(data),20)
saveResults("SPHERICAL KMEANS", NMI(res.sph_kmeans$cluster - 1,true_labels$V1), ARI(res.sph_kmeans$cluster - 1,true_labels$V1), "Les résultats de sphérical Kmeans sont pratiquement identiques (cela signifie que les deux algorithmes ont produit des clusters similaires en termes de qualité) à ceux obtenus avec Kmeans nous pouvons expliquer celà par le fait que : Cela peut être dû au fait que les données sont déjà sphériques ou qu'elles ont une distribution gaussienne, et que l'utilisation de l'algorithme Spherical k-means n'apporte donc pas de bénéfice supplémentaire par rapport à l'algorithme k-means standard. Il est également possible que les deux algorithmes ont convergé vers les mêmes résultats. De plus, la NMI n'est qu'une des métriques d'évaluation de la qualité de clustering, il est donc important de considérer d'autres indicateurs pour évaluer les résultats obtenus avec les deux algorithmes.")
View(algoFinal)
D = kNNdist(as.data.frame(data)  ,k=20,search="kd")
plot(sort(D,decreasing=TRUE),type="l",xlim=c(0,1000),ylim=c(0,10))
abline(h=0.15,col="red",lty=2)
x <- as.matrix(data)
res.dbscan <- dbscan(x, eps = 1.2, minPts = 10)
res.dbscan_nmi <- NMI(res.dbscan$cluster - 1,true_labels$V1)
res.dbscan_nmi
saveResults("DBSCAN", NMI(res.dbscan$cluster - 1,true_labels$V1), ARI(res.dbscan$cluster - 1,true_labels$V1), "Cet algorithme retourne soit 1 soit 2 clusters, nous pouvons dire qu'il faut utiliser d'autres techniques pour le meilleur choix des paramètre ou alors nos données ne sont pas suffisamment densément groupées or, l'algorithme se base sur la densité afin de retourner plusieurs clusters ou aucun cluster si les données ne remplissent pas les critères de densité nécessaires (qui pourrait probablement être notre cas).")
res.dbscan
dist <- as.vector(dist(data))
clara <- clara(data, 20, samples = 50, pamLike = TRUE)
clara_clust <- clara$clustering
saveResults("CLARA", NMI(as.factor(clara_clust), as.factor(true_labels$V1) ), ARI(as.factor(clara_clust), as.factor(true_labels$V1) ), "")
res.fastpam <- fastpam(dist,nrow(data), 20)
saveResults("FAST PAM", NMI(as.factor(res.fastpam@assignment - 1), as.factor(true_labels$V1) ), ARI(as.factor(res.fastpam@assignment - 1), as.factor(true_labels$V1) ), "")
res.fastclara <- fastclara(dist, nrow(data), k=20, numsamples = 5, sampling=0.25)
saveResults("FAST CLARA", NMI(as.factor(res.fastclara@assignment - 1), as.factor(true_labels$V1) ), ARI(as.factor(res.fastclara@assignment - 1), as.factor(true_labels$V1) ), "Nous pouvons donc constater que Fast Clara donne de meilleur résultats que Fast Clarans")
res.fastclarans <- fastclarans(dist, nrow(data), 20, numlocal=2, maxneighbor=0.025)
saveResults("FAST CLARANS", NMI(as.factor(res.fastclarans@assignment - 1), as.factor(true_labels$V1) ), ARI(as.factor(res.fastclarans@assignment - 1), as.factor(true_labels$V1) ), "Nous pouvons donc constater que Fast Clara donne de meilleur résultats que Fast Clarans")
#dat <-matrix(unlist(data), ncol=ncol(data), byrow=TRUE)
#result <- specc(dat, centers = 20)
#saveResults("SPECTRAL CLUSTERING", NMI(as.factor(result), as.factor(true_labels$V1)), ARI(as.factor(result), as.factor(true_labels$V1)), "")
saveResults("SPECTRAL CLUSTERING", 0, 0, "L'algorithme Spectral clustering tourne des heures sans jamais s'arrêter, nous l'avons lancé toute une nuit sans obtenir de résultats le problème peut être du à : La taille des données : Si les données sont très volumineuses, l'algorithme peut prendre beaucoup de temps, le nombre de clusters demandé est très élevé, cela peut rendre l'algorithme très complexe et prendre beaucoup de temps pour s'exécuter.")
r <- RSKC(data, ncl = 20, alpha = 0.1, nstart = 10)
saveResults("SPARSE KMEANS", NMI(as.factor(r$labels), as.factor(true_labels$V1) ), ARI(as.factor(r$labels), as.factor(true_labels$V1)), "Sparse Kmeans donne de bons résultats, nous pouvons expliquer celà par le fait que le partitionnement des données permet de mieux gérer les outliers et les points atypiques, ce qui améliore la qualité des clusters formés.le partitionnement peut également aider à identifier des structures non linéaires dans les données, ce qui améliore la qualité des clusters formés.")
data.umap <- umap(data, n_neighbors = 20, min_dist = 0.0001)
res.umap <- data.umap$layout %>% as.data.frame()%>% rename(UMAP1="V1", UMAP2="V2")
data.kmeans.umap <- kmeans(res.umap ,20)
saveResults("UMAP + KMEANS", NMI(as.factor(data.kmeans.umap$cluster - 1), as.factor(true_labels$V1) ), ARI(as.factor(data.kmeans.umap$cluster - 1), as.factor(true_labels$V1) ), "UMAP + Kmeans fonctionne mieux que ACP + Kmeans car UMAP est un algorithme de réduction de dimension qui est spécifiquement conçu pour conserver les propriétés de similarité des données d'origine tout en réduisant les dimensions. Il utilise un processus de mappage uniforme pour projeter les données dans un espace de dimension réduite, en gardant les observations similaires proches les unes des autres dans la projection.
En revanche, l'ACP est un algorithme de réduction de dimension qui se concentre sur la capture de la variation maximale des données d'origine dans un nombre limité de dimensions. Il peut ne pas conserver les propriétés de similarité des données d'origine de la même manière que UMAP, ce qui peut entraîner une perte d'informations importante ceci pourrait expliquer le fait que les résultats obtenus avec UMAP sont meilleurs.")
fviz_cluster(data.kmeans.umap, data = res.umap, geom="point")
NMI(data.kmeans.umap$cluster - 1,true_labels$V1)
data.acp <- PCA(data, ncp = 15, graph = TRUE)
data.kmeans.acp <- kmeans(data.acp$call$X ,20)
saveResults("ACP + KMEANS", NMI(as.factor(data.kmeans.acp$cluster - 1), as.factor(true_labels$V1) ), ARI(as.factor(data.kmeans.acp$cluster - 1), as.factor(true_labels$V1) ), "UMAP + Kmeans fonctionne mieux que ACP + Kmeans car UMAP est un algorithme de réduction de dimension qui est spécifiquement conçu pour conserver les propriétés de similarité des données d'origine tout en réduisant les dimensions. Il utilise un processus de mappage uniforme pour projeter les données dans un espace de dimension réduite, en gardant les observations similaires proches les unes des autres dans la projection.
En revanche, l'ACP est un algorithme de réduction de dimension qui se concentre sur la capture de la variation maximale des données d'origine dans un nombre limité de dimensions. Il peut ne pas conserver les propriétés de similarité des données d'origine de la même manière que UMAP, ce qui peut entraîner une perte d'informations importante ceci pourrait expliquer le fait que les résultats obtenus avec UMAP sont meilleurs.")
fviz_cluster(data.kmeans.acp, data = data.acp$call$X)
plot(data.acp$call$X[,1:2], col=data.kmeans.acp$cluster)
points(data.kmeans.acp$centers[,1:2], col=1:3, pch=8, cex=3)
BIC <- mclustBIC(data, G = 20)
print("BIC pour le meilleur modèle donne: ")
summary(BIC, parameters = TRUE)
mclust = Mclust(data, x = BIC)
layers <- 2
k <- c(3, 4)
r <- c(3, 2)
it <- 50
eps <- 0.001
set.seed(1)
fit <-deepgmm(y = data, layers = layers, k = k, r = r,it = it, eps = eps)
summary(fit)
clusters <- cluspca(data, 20, method = "RKM")
saveResults("REDUCED KMEANS + ACP", NMI(as.factor(clusters$cluster - 1), as.factor(true_labels$V1) ), ARI(as.factor(clusters$cluster - 1), as.factor(true_labels$V1) ), "Reduced Kmeans donne pratiquement les même résultats que Kmeans malgré qu'il applique une ACP, nous pouvons expliquer celà par le fait que l'ACP ne conserve pas les propriétés de similarité des données d'origine, elle se concentre sur la capture de la variation maximale des données d'origine dans un nombre limité de dimensions")
algoFinal
BIC <- mclustBIC(data, G = 20)
print("BIC pour le meilleur modèle donne: ")
summary(BIC, parameters = TRUE)
mclust = Mclust(data, x = BIC)
plot(mclust$classification)
print(paste("NMI", NMI(as.factor(clusters$cluster - 1))))
print(paste("NMI", NMI(as.factor(clusters$cluster - 1)), as.factor(true_labels$V1)))
print(paste("NMI", NMI(as.factor(clusters$cluster - 1), as.factor(true_labels$V1) )))
print(paste("NMI", ARI(as.factor(clusters$cluster - 1), as.factor(true_labels$V1) )))
print(paste("NMI", NMI(as.factor(clusters$cluster - 1), as.factor(true_labels$V1) )))
print(paste("ARI", ARI(as.factor(clusters$cluster - 1), as.factor(true_labels$V1) )))
algoFinal
library(shiny); runApp('GitHub/declaration-automation-base-on-R/projet_fin.R')
library("readxl")
library("FactoMineR")
library("factoextra")
library(Factoshiny)
library(NbClust)
library(caret)
library(mclust)
library(ggplot2)
library(stats)
library(factoextra)
library(fpc)
library(cluster)
#library(spectralCluster)
library(grid)
library(kernlab)
library(dbscan)
library(aricode)
library(skmeans)
library(fastkmedoids)
library("umap")
library(cluster)
library(dplyr)
library(clustrd)
library(RSKC)
View(data)
library(EMMIXmfa)
library(deepgmm)
algoFinal <- data.frame(matrix(ncol = 4))
columns <- c(
"NOM ALGORITHME",	"NMI",	"ARI", "COMMENTAIRE"
)
colnames(algoFinal) <- columns
current_algorihtm <- algoFinal
algoFinal <- algoFinal[-1,]
saveResults <- function(nomAlgo, nmi, ari, commnt){
current_algorihtm$`NOM ALGORITHME` <- nomAlgo
current_algorihtm$NMI <- round(nmi,2)
current_algorihtm$ARI <- round(ari,2)
current_algorihtm$COMMENTAIRE <- commnt
#sauvegarder les résultats obtenus
algoFinal <<- rbind(algoFinal, current_algorihtm)
#réinitailiser la variable
current_algorihtm <- data.frame(matrix(ncol = 4))
colnames(current_algorihtm) <- columns
}
data <- read.table("jose_d.txt", sep = "", na.strings = "", stringsAsFactors = F, skip = 1, fill = T)
data <- data[,2:ncol(data)]
data <- data[,2:ncol(data)]
head(data)
library(shiny); runApp('GitHub/declaration-automation-base-on-R/projet_fin.R')
View(dataForGivenYear)
View(StocksdataFrame)
runApp('GitHub/declaration-automation-base-on-R/projet_fin.R')
library(shiny); runApp('projet_fin.R')
